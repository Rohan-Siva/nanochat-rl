{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "b9403e9b",
            "metadata": {},
            "source": [
                "# NanoChat Inference & CUDA Check\n",
                "\n",
                "This notebook checks for CUDA availability, downloads the model artifacts, and runs a simple inference using the `nanochat` model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "240909d6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch version: 2.9.1+cu128\n",
                        "CUDA available: True\n",
                        "Device name: NVIDIA RTX A6000\n",
                        "Device count: 4\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
                "else:\n",
                "    print(\"CUDA is NOT available. Running on CPU or other device.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "39445060",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Downloading https://huggingface.co/karpathy/nanochat-d32/resolve/main/token_bytes.pt to /home/rs63759/.cache/nanochat/tokenizer/token_bytes.pt...\n",
                        "Downloaded /home/rs63759/.cache/nanochat/tokenizer/token_bytes.pt\n",
                        "Downloading https://huggingface.co/karpathy/nanochat-d32/resolve/main/tokenizer.pkl to /home/rs63759/.cache/nanochat/tokenizer/tokenizer.pkl...\n",
                        "Downloaded /home/rs63759/.cache/nanochat/tokenizer/tokenizer.pkl\n",
                        "Downloading https://huggingface.co/karpathy/nanochat-d32/resolve/main/meta_000650.json to /home/rs63759/.cache/nanochat/chatsft_checkpoints/d32/meta_000650.json...\n",
                        "Downloaded /home/rs63759/.cache/nanochat/chatsft_checkpoints/d32/meta_000650.json\n",
                        "Downloading https://huggingface.co/karpathy/nanochat-d32/resolve/main/model_000650.pt to /home/rs63759/.cache/nanochat/chatsft_checkpoints/d32/model_000650.pt...\n",
                        "Downloaded /home/rs63759/.cache/nanochat/chatsft_checkpoints/d32/model_000650.pt\n",
                        "All files downloaded successfully.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import requests\n",
                "from pathlib import Path\n",
                "\n",
                "def download_file(url, dest_path):\n",
                "    if os.path.exists(dest_path):\n",
                "        print(f\"File already exists: {dest_path}\")\n",
                "        return\n",
                "    print(f\"Downloading {url} to {dest_path}...\")\n",
                "    response = requests.get(url, stream=True)\n",
                "    response.raise_for_status()\n",
                "    with open(dest_path, 'wb') as f:\n",
                "        for chunk in response.iter_content(chunk_size=8192):\n",
                "            f.write(chunk)\n",
                "    print(f\"Downloaded {dest_path}\")\n",
                "\n",
                "home = Path.home()\n",
                "cache_dir = home / \".cache\" / \"nanochat\"\n",
                "tokenizer_dir = cache_dir / \"tokenizer\"\n",
                "checkpoints_dir = cache_dir / \"chatsft_checkpoints\" / \"d32\"\n",
                "\n",
                "tokenizer_dir.mkdir(parents=True, exist_ok=True)\n",
                "checkpoints_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "base_url = \"https://huggingface.co/karpathy/nanochat-d32/resolve/main\"\n",
                "\n",
                "download_file(f\"{base_url}/token_bytes.pt\", tokenizer_dir / \"token_bytes.pt\")\n",
                "download_file(f\"{base_url}/tokenizer.pkl\", tokenizer_dir / \"tokenizer.pkl\")\n",
                "\n",
                "download_file(f\"{base_url}/meta_000650.json\", checkpoints_dir / \"meta_000650.json\")\n",
                "download_file(f\"{base_url}/model_000650.pt\", checkpoints_dir / \"model_000650.pt\")\n",
                "\n",
                "print(\"All files downloaded successfully.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "45eef9b8",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-11-23 19:41:50,143 - nanochat.common - \u001b[32m\u001b[1mINFO\u001b[0m - Distributed world size: 1\n",
                        "2025-11-23 19:41:50,146 - nanochat.checkpoint_manager - \u001b[32m\u001b[1mINFO\u001b[0m - Loading model from /home/rs63759/.cache/nanochat/chatsft_checkpoints/d32 with step 650\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Initializing...\n",
                        "Autodetected device type: cuda\n",
                        "Using device: cuda\n",
                        "Error loading model: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 6.19 MiB is free. Process 2859735 has 46.37 GiB memory in use. Including non-PyTorch memory, this process has 1.00 GiB memory in use. Of the allocated memory 768.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
                        "Make sure you have trained a model or downloaded checkpoints.\n"
                    ]
                },
                {
                    "ename": "OutOfMemoryError",
                    "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 6.19 MiB is free. Process 2859735 has 46.37 GiB memory in use. Including non-PyTorch memory, this process has 1.00 GiB memory in use. Of the allocated memory 768.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[5], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure you have trained a model or downloaded checkpoints.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Stop execution if model fails to load\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     40\u001b[0m engine \u001b[38;5;241m=\u001b[39m Engine(model, tokenizer)\n",
                        "Cell \u001b[0;32mIn[5], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Load Model\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# We explicitly set model_tag to \"d32\" to match the directory we created\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     model, tokenizer, meta \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSOURCE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_tag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
                        "File \u001b[0;32m~/code/nanochat-rl/nanochat/checkpoint_manager.py:151\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(source, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m get_base_dir()\n\u001b[1;32m    150\u001b[0m checkpoints_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, model_dir)\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoints_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/code/nanochat-rl/nanochat/checkpoint_manager.py:139\u001b[0m, in \u001b[0;36mload_model_from_dir\u001b[0;34m(checkpoints_dir, device, phase, model_tag, step)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# build the model\u001b[39;00m\n\u001b[1;32m    138\u001b[0m log0(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m model, tokenizer, meta_data \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer, meta_data\n",
                        "File \u001b[0;32m~/code/nanochat-rl/nanochat/checkpoint_manager.py:66\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(checkpoint_dir, step, device, phase)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mA bunch of repetitive code to build a model from a given checkpoint.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m- meta data saved during base model training\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m phase \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid phase: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 66\u001b[0m model_data, optimizer_data, meta_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Convert bfloat16 tensors to float for CPU inference\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     model_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     70\u001b[0m         k: v\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16 \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m model_data\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     72\u001b[0m     }\n",
                        "File \u001b[0;32m~/code/nanochat-rl/nanochat/checkpoint_manager.py:44\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[0;34m(checkpoint_dir, step, device, load_optimizer, rank)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_checkpoint\u001b[39m(checkpoint_dir, step, device, load_optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Load the model state\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m06d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m     model_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Load the optimizer state if requested\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     optimizer_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniforge3/envs/nanochat-rl/lib/python3.10/site-packages/torch/serialization.py:1521\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1528\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1529\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniforge3/envs/nanochat-rl/lib/python3.10/site-packages/torch/serialization.py:2122\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   2121\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 2122\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2123\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2125\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
                        "File \u001b[0;32m~/miniforge3/envs/nanochat-rl/lib/python3.10/site-packages/torch/_weights_only_unpickler.py:535\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    530\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m     ):\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[1;32m    533\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         )\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[1;32m    537\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
                        "File \u001b[0;32m~/miniforge3/envs/nanochat-rl/lib/python3.10/site-packages/torch/serialization.py:2086\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2085\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 2086\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
                        "File \u001b[0;32m~/miniforge3/envs/nanochat-rl/lib/python3.10/site-packages/torch/serialization.py:2052\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mdetect_fake_mode(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     wrap_storage \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2054\u001b[0m     storage\u001b[38;5;241m.\u001b[39m_fake_device \u001b[38;5;241m=\u001b[39m location\n",
                        "File \u001b[0;32m~/miniforge3/envs/nanochat-rl/lib/python3.10/site-packages/torch/serialization.py:1864\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m-> 1864\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniforge3/envs/nanochat-rl/lib/python3.10/site-packages/torch/serialization.py:698\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 698\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
                        "File \u001b[0;32m~/miniforge3/envs/nanochat-rl/lib/python3.10/site-packages/torch/serialization.py:637\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m    636\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniforge3/envs/nanochat-rl/lib/python3.10/site-packages/torch/storage.py:291\u001b[0m, in \u001b[0;36m_StorageBase.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, torch\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m    290\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device)\n\u001b[0;32m--> 291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniforge3/envs/nanochat-rl/lib/python3.10/site-packages/torch/_utils.py:101\u001b[0m, in \u001b[0;36m_to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sparse, (\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse storage is not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     )\n\u001b[0;32m--> 101\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 6.19 MiB is free. Process 2859735 has 46.37 GiB memory in use. Including non-PyTorch memory, this process has 1.00 GiB memory in use. Of the allocated memory 768.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "if os.getcwd() not in sys.path:\n",
                "    sys.path.append(os.getcwd())\n",
                "\n",
                "from nanochat.common import compute_init, autodetect_device_type\n",
                "from nanochat.checkpoint_manager import load_model\n",
                "from nanochat.engine import Engine\n",
                "from contextlib import nullcontext\n",
                "\n",
                "SOURCE = \"sft\"  # sft/mid/rl options\n",
                "DEVICE_TYPE = \"\" # default is cuda\n",
                "DTYPE = \"bfloat16\" \n",
                "\n",
                "print(\"Initializing...\")\n",
                "\n",
                "# setup device\n",
                "device_type = autodetect_device_type() if DEVICE_TYPE == \"\" else DEVICE_TYPE\n",
                "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n",
                "\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "ptdtype = torch.float32 if DTYPE == 'float32' else torch.bfloat16\n",
                "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n",
                "\n",
                "# load model\n",
                "try:\n",
                "    model, tokenizer, meta = load_model(SOURCE, device, phase=\"eval\", model_tag=\"d32\")\n",
                "    print(\"Model loaded successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"Error loading model: {e}\")\n",
                "    print(\"Make sure you have trained a model or downloaded checkpoints.\")\n",
                "    raise e\n",
                "\n",
                "engine = Engine(model, tokenizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def chat(prompt, temperature=0.6, top_k=50, max_tokens=256):\n",
                "    \"\"\"Runs a single turn of chat inference.\"\"\"\n",
                "    \n",
                "    bos = tokenizer.get_bos_token_id()\n",
                "    user_start = tokenizer.encode_special(\"<|user_start|>\")\n",
                "    user_end = tokenizer.encode_special(\"<|user_end|>\")\n",
                "    assistant_start = tokenizer.encode_special(\"<|assistant_start|>\")\n",
                "    assistant_end = tokenizer.encode_special(\"<|assistant_end|>\")\n",
                "    \n",
                "    conversation_tokens = [bos]\n",
                "    conversation_tokens.append(user_start)\n",
                "    conversation_tokens.extend(tokenizer.encode(prompt))\n",
                "    conversation_tokens.append(user_end)\n",
                "    conversation_tokens.append(assistant_start)\n",
                "    \n",
                "    generate_kwargs = {\n",
                "        \"num_samples\": 1,\n",
                "        \"max_tokens\": max_tokens,\n",
                "        \"temperature\": temperature,\n",
                "        \"top_k\": top_k,\n",
                "    }\n",
                "    \n",
                "    print(f\"User: {prompt}\")\n",
                "    print(\"Assistant: \", end=\"\", flush=True)\n",
                "    \n",
                "    response_tokens = []\n",
                "    with autocast_ctx:\n",
                "        for token_column, token_masks in engine.generate(conversation_tokens, **generate_kwargs):\n",
                "            token = token_column[0]\n",
                "            response_tokens.append(token)\n",
                "            token_text = tokenizer.decode([token])\n",
                "            print(token_text, end=\"\", flush=True)\n",
                "    print(\"\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chat(\"Hello! Who are you?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chat(\"Write a short poem about coding.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nanochat-rl",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
