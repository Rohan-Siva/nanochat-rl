{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "b9403e9b",
            "metadata": {},
            "source": [
                "# NanoChat Inference & CUDA Check\n",
                "\n",
                "This notebook checks for CUDA availability, downloads the model artifacts, and runs a simple inference using the `nanochat` model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "240909d6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch version: 2.9.1+cu128\n",
                        "CUDA available: True\n",
                        "Device name: NVIDIA RTX A6000\n",
                        "Device count: 3\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
                "else:\n",
                "    print(\"CUDA is NOT available. Running on CPU or other device.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "39445060",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Downloading https://huggingface.co/karpathy/nanochat-d32/resolve/main/token_bytes.pt to /home/jl77863/.cache/nanochat/tokenizer/token_bytes.pt...\n",
                        "Downloaded /home/jl77863/.cache/nanochat/tokenizer/token_bytes.pt\n",
                        "Downloading https://huggingface.co/karpathy/nanochat-d32/resolve/main/tokenizer.pkl to /home/jl77863/.cache/nanochat/tokenizer/tokenizer.pkl...\n",
                        "Downloaded /home/jl77863/.cache/nanochat/tokenizer/tokenizer.pkl\n",
                        "Downloading https://huggingface.co/karpathy/nanochat-d32/resolve/main/meta_000650.json to /home/jl77863/.cache/nanochat/chatsft_checkpoints/d32/meta_000650.json...\n",
                        "Downloaded /home/jl77863/.cache/nanochat/chatsft_checkpoints/d32/meta_000650.json\n",
                        "Downloading https://huggingface.co/karpathy/nanochat-d32/resolve/main/model_000650.pt to /home/jl77863/.cache/nanochat/chatsft_checkpoints/d32/model_000650.pt...\n",
                        "Downloaded /home/jl77863/.cache/nanochat/chatsft_checkpoints/d32/model_000650.pt\n",
                        "All files downloaded successfully.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import requests\n",
                "from pathlib import Path\n",
                "\n",
                "def download_file(url, dest_path):\n",
                "    if os.path.exists(dest_path):\n",
                "        print(f\"File already exists: {dest_path}\")\n",
                "        return\n",
                "    print(f\"Downloading {url} to {dest_path}...\")\n",
                "    response = requests.get(url, stream=True)\n",
                "    response.raise_for_status()\n",
                "    with open(dest_path, 'wb') as f:\n",
                "        for chunk in response.iter_content(chunk_size=8192):\n",
                "            f.write(chunk)\n",
                "    print(f\"Downloaded {dest_path}\")\n",
                "\n",
                "home = Path.home()\n",
                "cache_dir = home / \".cache\" / \"nanochat\"\n",
                "tokenizer_dir = cache_dir / \"tokenizer\"\n",
                "checkpoints_dir = cache_dir / \"chatsft_checkpoints\" / \"d32\"\n",
                "\n",
                "tokenizer_dir.mkdir(parents=True, exist_ok=True)\n",
                "checkpoints_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "base_url = \"https://huggingface.co/karpathy/nanochat-d32/resolve/main\"\n",
                "\n",
                "download_file(f\"{base_url}/token_bytes.pt\", tokenizer_dir / \"token_bytes.pt\")\n",
                "download_file(f\"{base_url}/tokenizer.pkl\", tokenizer_dir / \"tokenizer.pkl\")\n",
                "\n",
                "download_file(f\"{base_url}/meta_000650.json\", checkpoints_dir / \"meta_000650.json\")\n",
                "download_file(f\"{base_url}/model_000650.pt\", checkpoints_dir / \"model_000650.pt\")\n",
                "\n",
                "print(\"All files downloaded successfully.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "45eef9b8",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-11-23 20:40:48,814 - nanochat.common - \u001b[32m\u001b[1mINFO\u001b[0m - Distributed world size: 1\n",
                        "2025-11-23 20:40:48,816 - nanochat.checkpoint_manager - \u001b[32m\u001b[1mINFO\u001b[0m - Loading model from /home/jl77863/.cache/nanochat/chatsft_checkpoints/d32 with step 650\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Initializing...\n",
                        "Autodetected device type: cuda\n",
                        "Using device: cuda\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-11-23 20:40:54,476 - nanochat.checkpoint_manager - \u001b[32m\u001b[1mINFO\u001b[0m - Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 32, 'n_head': 16, 'n_kv_head': 16, 'n_embd': 2048}\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model loaded successfully.\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "if os.getcwd() not in sys.path:\n",
                "    sys.path.append(os.getcwd())\n",
                "\n",
                "from nanochat.common import compute_init, autodetect_device_type\n",
                "from nanochat.checkpoint_manager import load_model\n",
                "from nanochat.engine import Engine\n",
                "from contextlib import nullcontext\n",
                "\n",
                "SOURCE = \"sft\"  # sft/mid/rl options\n",
                "DEVICE_TYPE = \"\" # default is cuda\n",
                "DTYPE = \"bfloat16\" \n",
                "\n",
                "print(\"Initializing...\")\n",
                "\n",
                "# setup device\n",
                "device_type = autodetect_device_type() if DEVICE_TYPE == \"\" else DEVICE_TYPE\n",
                "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n",
                "\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "ptdtype = torch.float32 if DTYPE == 'float32' else torch.bfloat16\n",
                "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n",
                "\n",
                "# load model\n",
                "try:\n",
                "    model, tokenizer, meta = load_model(SOURCE, device, phase=\"eval\", model_tag=\"d32\")\n",
                "    print(\"Model loaded successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"Error loading model: {e}\")\n",
                "    print(\"Make sure you have trained a model or downloaded checkpoints.\")\n",
                "    raise e\n",
                "\n",
                "engine = Engine(model, tokenizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def chat(prompt, temperature=0.6, top_k=50, max_tokens=256):\n",
                "    \"\"\"Runs a single turn of chat inference.\"\"\"\n",
                "    \n",
                "    bos = tokenizer.get_bos_token_id()\n",
                "    user_start = tokenizer.encode_special(\"<|user_start|>\")\n",
                "    user_end = tokenizer.encode_special(\"<|user_end|>\")\n",
                "    assistant_start = tokenizer.encode_special(\"<|assistant_start|>\")\n",
                "    assistant_end = tokenizer.encode_special(\"<|assistant_end|>\")\n",
                "    \n",
                "    conversation_tokens = [bos]\n",
                "    conversation_tokens.append(user_start)\n",
                "    conversation_tokens.extend(tokenizer.encode(prompt))\n",
                "    conversation_tokens.append(user_end)\n",
                "    conversation_tokens.append(assistant_start)\n",
                "    \n",
                "    generate_kwargs = {\n",
                "        \"num_samples\": 1,\n",
                "        \"max_tokens\": max_tokens,\n",
                "        \"temperature\": temperature,\n",
                "        \"top_k\": top_k,\n",
                "    }\n",
                "    \n",
                "    print(f\"User: {prompt}\")\n",
                "    print(\"Assistant: \", end=\"\", flush=True)\n",
                "    \n",
                "    response_tokens = []\n",
                "    with autocast_ctx:\n",
                "        for token_column, token_masks in engine.generate(conversation_tokens, **generate_kwargs):\n",
                "            token = token_column[0]\n",
                "            response_tokens.append(token)\n",
                "            token_text = tokenizer.decode([token])\n",
                "            print(token_text, end=\"\", flush=True)\n",
                "    print(\"\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "User: Hello! Who are you?\n",
                        "Assistant: I'm a chatbot that speaks in a friendly, conversational tone. I'm here to help you with any questions or issues you might be facing. I don't have a physical presence, but I'm here to listen and provide support. Feel free to share your concerns, ask questions, or seek advice, and I'll do my best to help. What's on your mind today?<|assistant_end|>\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "chat(\"Hello! Who are you?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "User: Write a short poem about coding.\n",
                        "Assistant: In the realm of lines and symbols,\n",
                        "A language of code, for all to know,\n",
                        "From lowly programmers to the great,\n",
                        "The code is the code, it's the same.\n",
                        "\n",
                        "Here, a programmer's work begins,\n",
                        "In the digital world, where code does reside,\n",
                        "The code is the secret key,\n",
                        "To unlock the secrets of the computer.\n",
                        "\n",
                        "It's not just about writing code,\n",
                        "It's about solving problems too,\n",
                        "The code is the solution's name,\n",
                        "The final touch, that makes it complete.\n",
                        "\n",
                        "Yet, the code can be wrong,\n",
                        "And flaws can appear, in the code's design,\n",
                        "But that's not a problem, for the code's sake,\n",
                        "For it's the code's purpose, to make things happen.\n",
                        "\n",
                        "So, the next time you write code,\n",
                        "Think of the code as your friend,\n",
                        "For it's the code that builds the world,\n",
                        "And makes the digital world, our home.<|assistant_end|>\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "chat(\"Write a short poem about coding.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nanochat-rl",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
