{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "b9403e9b",
            "metadata": {},
            "source": [
                "# NanoChat Inference & CUDA Check\n",
                "\n",
                "This notebook checks for CUDA availability, downloads the model artifacts, and runs a simple inference using the `nanochat` model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "240909d6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch version: 2.9.1+cu128\n",
                        "CUDA available: True\n",
                        "Device name: NVIDIA RTX A6000\n",
                        "Device count: 4\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
                "else:\n",
                "    print(\"CUDA is NOT available. Running on CPU or other device.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "39445060",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Downloading https://huggingface.co/karpathy/nanochat-d32/resolve/main/token_bytes.pt to /home/rs63759/.cache/nanochat/tokenizer/token_bytes.pt...\n",
                        "Downloaded /home/rs63759/.cache/nanochat/tokenizer/token_bytes.pt\n",
                        "Downloading https://huggingface.co/karpathy/nanochat-d32/resolve/main/tokenizer.pkl to /home/rs63759/.cache/nanochat/tokenizer/tokenizer.pkl...\n",
                        "Downloaded /home/rs63759/.cache/nanochat/tokenizer/tokenizer.pkl\n",
                        "Downloading https://huggingface.co/karpathy/nanochat-d32/resolve/main/meta_000650.json to /home/rs63759/.cache/nanochat/chatsft_checkpoints/d32/meta_000650.json...\n",
                        "Downloaded /home/rs63759/.cache/nanochat/chatsft_checkpoints/d32/meta_000650.json\n",
                        "Downloading https://huggingface.co/karpathy/nanochat-d32/resolve/main/model_000650.pt to /home/rs63759/.cache/nanochat/chatsft_checkpoints/d32/model_000650.pt...\n",
                        "Downloaded /home/rs63759/.cache/nanochat/chatsft_checkpoints/d32/model_000650.pt\n",
                        "All files downloaded successfully.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import requests\n",
                "from pathlib import Path\n",
                "\n",
                "def download_file(url, dest_path):\n",
                "    if os.path.exists(dest_path):\n",
                "        print(f\"File already exists: {dest_path}\")\n",
                "        return\n",
                "    print(f\"Downloading {url} to {dest_path}...\")\n",
                "    response = requests.get(url, stream=True)\n",
                "    response.raise_for_status()\n",
                "    with open(dest_path, 'wb') as f:\n",
                "        for chunk in response.iter_content(chunk_size=8192):\n",
                "            f.write(chunk)\n",
                "    print(f\"Downloaded {dest_path}\")\n",
                "\n",
                "home = Path.home()\n",
                "cache_dir = home / \".cache\" / \"nanochat\"\n",
                "tokenizer_dir = cache_dir / \"tokenizer\"\n",
                "checkpoints_dir = cache_dir / \"chatsft_checkpoints\" / \"d32\"\n",
                "\n",
                "tokenizer_dir.mkdir(parents=True, exist_ok=True)\n",
                "checkpoints_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "base_url = \"https://huggingface.co/karpathy/nanochat-d32/resolve/main\"\n",
                "\n",
                "download_file(f\"{base_url}/token_bytes.pt\", tokenizer_dir / \"token_bytes.pt\")\n",
                "download_file(f\"{base_url}/tokenizer.pkl\", tokenizer_dir / \"tokenizer.pkl\")\n",
                "\n",
                "download_file(f\"{base_url}/meta_000650.json\", checkpoints_dir / \"meta_000650.json\")\n",
                "download_file(f\"{base_url}/model_000650.pt\", checkpoints_dir / \"model_000650.pt\")\n",
                "\n",
                "print(\"All files downloaded successfully.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "45eef9b8",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "if os.getcwd() not in sys.path:\n",
                "    sys.path.append(os.getcwd())\n",
                "\n",
                "from nanochat.common import compute_init, autodetect_device_type\n",
                "from nanochat.checkpoint_manager import load_model\n",
                "from nanochat.engine import Engine\n",
                "from contextlib import nullcontext\n",
                "\n",
                "SOURCE = \"sft\"  # sft/mid/rl options\n",
                "DEVICE_TYPE = \"\" # default is cuda\n",
                "DTYPE = \"bfloat16\" \n",
                "\n",
                "print(\"Initializing...\")\n",
                "\n",
                "# setup device\n",
                "device_type = autodetect_device_type() if DEVICE_TYPE == \"\" else DEVICE_TYPE\n",
                "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n",
                "\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "ptdtype = torch.float32 if DTYPE == 'float32' else torch.bfloat16\n",
                "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n",
                "\n",
                "# load model\n",
                "try:\n",
                "    model, tokenizer, meta = load_model(SOURCE, device, phase=\"eval\", model_tag=\"d32\")\n",
                "    print(\"Model loaded successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"Error loading model: {e}\")\n",
                "    print(\"Make sure you have trained a model or downloaded checkpoints.\")\n",
                "    raise e\n",
                "\n",
                "engine = Engine(model, tokenizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def chat(prompt, temperature=0.6, top_k=50, max_tokens=256):\n",
                "    \"\"\"Runs a single turn of chat inference.\"\"\"\n",
                "    \n",
                "    bos = tokenizer.get_bos_token_id()\n",
                "    user_start = tokenizer.encode_special(\"<|user_start|>\")\n",
                "    user_end = tokenizer.encode_special(\"<|user_end|>\")\n",
                "    assistant_start = tokenizer.encode_special(\"<|assistant_start|>\")\n",
                "    assistant_end = tokenizer.encode_special(\"<|assistant_end|>\")\n",
                "    \n",
                "    conversation_tokens = [bos]\n",
                "    conversation_tokens.append(user_start)\n",
                "    conversation_tokens.extend(tokenizer.encode(prompt))\n",
                "    conversation_tokens.append(user_end)\n",
                "    conversation_tokens.append(assistant_start)\n",
                "    \n",
                "    generate_kwargs = {\n",
                "        \"num_samples\": 1,\n",
                "        \"max_tokens\": max_tokens,\n",
                "        \"temperature\": temperature,\n",
                "        \"top_k\": top_k,\n",
                "    }\n",
                "    \n",
                "    print(f\"User: {prompt}\")\n",
                "    print(\"Assistant: \", end=\"\", flush=True)\n",
                "    \n",
                "    response_tokens = []\n",
                "    with autocast_ctx:\n",
                "        for token_column, token_masks in engine.generate(conversation_tokens, **generate_kwargs):\n",
                "            token = token_column[0]\n",
                "            response_tokens.append(token)\n",
                "            token_text = tokenizer.decode([token])\n",
                "            print(token_text, end=\"\", flush=True)\n",
                "    print(\"\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chat(\"Hello! Who are you?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chat(\"Write a short poem about coding.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nanochat-rl",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
