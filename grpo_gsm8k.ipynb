{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GRPO RL for gsm8k"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "07020eb7",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# !pip install -q datasets torch wandb\n",
                "# !pip install -e ."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "cf630b00",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Current working directory: /home/jl77863/nanochat-rl\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import itertools\n",
                "import torch\n",
                "import wandb\n",
                "from nanochat.common import compute_init, compute_cleanup, print0, get_base_dir, DummyWandb\n",
                "from nanochat.checkpoint_manager import save_checkpoint, load_model\n",
                "from nanochat.engine import Engine\n",
                "from tasks.gsm8k import GSM8K\n",
                "\n",
                "if os.path.basename(os.getcwd()) == \"scripts\":\n",
                "    os.chdir(\"..\")\n",
                "\n",
                "print(f\"Current working directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "65e72492",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "Finishing previous runs because reinit is set to 'default'."
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lrm</td><td>▁</td></tr><tr><td>reward</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lrm</td><td>1</td></tr><tr><td>reward</td><td>0.0625</td></tr><tr><td>step</td><td>0</td></tr></table><br/></div></div>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run <strong style=\"color:#cdcd00\">grpo_gsm8k_notebook</strong> at: <a href='https://wandb.ai/rohansiva-ut-austin/nanochat-rl/runs/0z7r9z8h' target=\"_blank\">https://wandb.ai/rohansiva-ut-austin/nanochat-rl/runs/0z7r9z8h</a><br> View project at: <a href='https://wandb.ai/rohansiva-ut-austin/nanochat-rl' target=\"_blank\">https://wandb.ai/rohansiva-ut-austin/nanochat-rl</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Find logs at: <code>./wandb/run-20251123_224617-0z7r9z8h/logs</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.23.0"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>/home/jl77863/nanochat-rl/wandb/run-20251123_225959-wspau9vo</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Syncing run <strong><a href='https://wandb.ai/rohansiva-ut-austin/nanochat-rl/runs/wspau9vo' target=\"_blank\">grpo_gsm8k_notebook</a></strong> to <a href='https://wandb.ai/rohansiva-ut-austin/nanochat-rl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/rohansiva-ut-austin/nanochat-rl' target=\"_blank\">https://wandb.ai/rohansiva-ut-austin/nanochat-rl</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/rohansiva-ut-austin/nanochat-rl/runs/wspau9vo' target=\"_blank\">https://wandb.ai/rohansiva-ut-austin/nanochat-rl/runs/wspau9vo</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# configs\n",
                "run_name = \"grpo_gsm8k_notebook\" \n",
                "source = \"sft\" \n",
                "dtype = \"bfloat16\"\n",
                "device_batch_size = 4 \n",
                "examples_per_step = 4 \n",
                "num_samples = 4 \n",
                "max_new_tokens = 256\n",
                "temperature = 1.0\n",
                "top_k = 50\n",
                "unembedding_lr = 0.004\n",
                "embedding_lr = 0.2\n",
                "matrix_lr = 0.02\n",
                "weight_decay = 0.0\n",
                "init_lr_frac = 0.05\n",
                "num_epochs = 1\n",
                "save_every = 200\n",
                "eval_every = 20\n",
                "eval_examples = 50\n",
                "\n",
                "use_wandb = True # log wandb\n",
                "if use_wandb:\n",
                "    wandb.init(project=\"nanochat-rl\", name=run_name)\n",
                "else:\n",
                "    wandb_run = DummyWandb()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "42c3d031",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-11-23 23:00:23,406 - nanochat.common - \u001b[32m\u001b[1mINFO\u001b[0m - Distributed world size: 1\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Device: cuda\n"
                    ]
                }
            ],
            "source": [
                "# Init compute\n",
                "# compute_init handles DDP, but we'll assume rank 0 for notebook simplicity if not launched with torchrun.\n",
                "try:\n",
                "    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init()\n",
                "except Exception as e:\n",
                "    print(f\"compute_init failed (expected in notebook if not torchrun): {e}\")\n",
                "    print(\"Falling back to manual device selection.\")\n",
                "    ddp = False\n",
                "    ddp_rank = 0\n",
                "    ddp_local_rank = 0\n",
                "    ddp_world_size = 1\n",
                "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "print(f\"Device: {device}\")\n",
                "master_process = ddp_rank == 0\n",
                "pt_dtype = torch.float32 if dtype == 'float32' else torch.bfloat16\n",
                "autocast_ctx = torch.amp.autocast(device_type=\"cuda\", dtype=pt_dtype)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "17720537",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-11-23 23:00:27,121 - nanochat.checkpoint_manager - \u001b[32m\u001b[1mINFO\u001b[0m - No model tag provided, guessing model tag: d32\n",
                        "2025-11-23 23:00:27,123 - nanochat.checkpoint_manager - \u001b[32m\u001b[1mINFO\u001b[0m - Loading model from /home/jl77863/.cache/nanochat/chatsft_checkpoints/d32 with step 650\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading model from sft...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-11-23 23:00:32,619 - nanochat.checkpoint_manager - \u001b[32m\u001b[1mINFO\u001b[0m - Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 32, 'n_head': 16, 'n_kv_head': 16, 'n_embd': 2048}\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model loaded.\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Loading model from {source}...\")\n",
                "model, tokenizer, meta = load_model(source, device, phase=\"eval\")\n",
                "engine = Engine(model, tokenizer)\n",
                "print(\"Model loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "aaf27ad6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-11-23 22:46:28,299 - httpx - \u001b[32m\u001b[1mINFO\u001b[0m - HTTP Request: HEAD https://huggingface.co/datasets/openai/gsm8k/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
                        "2025-11-23 22:46:28,337 - httpx - \u001b[32m\u001b[1mINFO\u001b[0m - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/datasets/openai/gsm8k/e53f048856ff4f594e959d75785d2c2d37b678ee/README.md \"HTTP/1.1 200 OK\"\n",
                        "2025-11-23 22:46:28,395 - httpx - \u001b[32m\u001b[1mINFO\u001b[0m - HTTP Request: HEAD https://huggingface.co/datasets/openai/gsm8k/resolve/e53f048856ff4f594e959d75785d2c2d37b678ee/gsm8k.py \"HTTP/1.1 404 Not Found\"\n",
                        "2025-11-23 22:46:28,526 - httpx - \u001b[32m\u001b[1mINFO\u001b[0m - HTTP Request: HEAD https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/openai/gsm8k/openai/gsm8k.py \"HTTP/1.1 404 Not Found\"\n",
                        "2025-11-23 22:46:28,595 - httpx - \u001b[32m\u001b[1mINFO\u001b[0m - HTTP Request: HEAD https://huggingface.co/datasets/openai/gsm8k/resolve/e53f048856ff4f594e959d75785d2c2d37b678ee/.huggingface.yaml \"HTTP/1.1 404 Not Found\"\n",
                        "2025-11-23 22:46:28,702 - httpx - \u001b[32m\u001b[1mINFO\u001b[0m - HTTP Request: GET https://datasets-server.huggingface.co/info?dataset=openai/gsm8k \"HTTP/1.1 200 OK\"\n",
                        "2025-11-23 22:46:28,763 - httpx - \u001b[32m\u001b[1mINFO\u001b[0m - HTTP Request: HEAD https://huggingface.co/datasets/openai/gsm8k/resolve/e53f048856ff4f594e959d75785d2c2d37b678ee/dataset_infos.json \"HTTP/1.1 404 Not Found\"\n",
                        "2025-11-23 22:46:28,831 - httpx - \u001b[32m\u001b[1mINFO\u001b[0m - HTTP Request: HEAD https://huggingface.co/datasets/openai/gsm8k/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
                        "2025-11-23 22:46:28,844 - httpx - \u001b[32m\u001b[1mINFO\u001b[0m - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/datasets/openai/gsm8k/e53f048856ff4f594e959d75785d2c2d37b678ee/README.md \"HTTP/1.1 200 OK\"\n",
                        "2025-11-23 22:46:28,904 - httpx - \u001b[32m\u001b[1mINFO\u001b[0m - HTTP Request: HEAD https://huggingface.co/datasets/openai/gsm8k/resolve/e53f048856ff4f594e959d75785d2c2d37b678ee/gsm8k.py \"HTTP/1.1 404 Not Found\"\n",
                        "2025-11-23 22:46:28,950 - httpx - \u001b[32m\u001b[1mINFO\u001b[0m - HTTP Request: HEAD https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/openai/gsm8k/openai/gsm8k.py \"HTTP/1.1 404 Not Found\"\n",
                        "2025-11-23 22:46:29,018 - httpx - \u001b[32m\u001b[1mINFO\u001b[0m - HTTP Request: HEAD https://huggingface.co/datasets/openai/gsm8k/resolve/e53f048856ff4f594e959d75785d2c2d37b678ee/.huggingface.yaml \"HTTP/1.1 404 Not Found\"\n",
                        "2025-11-23 22:46:29,086 - httpx - \u001b[32m\u001b[1mINFO\u001b[0m - HTTP Request: GET https://datasets-server.huggingface.co/info?dataset=openai/gsm8k \"HTTP/1.1 200 OK\"\n",
                        "2025-11-23 22:46:29,142 - httpx - \u001b[32m\u001b[1mINFO\u001b[0m - HTTP Request: HEAD https://huggingface.co/datasets/openai/gsm8k/resolve/e53f048856ff4f594e959d75785d2c2d37b678ee/dataset_infos.json \"HTTP/1.1 404 Not Found\"\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training examples: 7473\n",
                        "Calculated number of steps: 1868\n"
                    ]
                }
            ],
            "source": [
                "# Initialize Tasks\n",
                "train_task = GSM8K(subset=\"main\", split=\"train\")\n",
                "val_task = GSM8K(subset=\"main\", split=\"test\")\n",
                "num_steps = (len(train_task) // examples_per_step) * num_epochs\n",
                "print(f\"Training examples: {len(train_task)}\")\n",
                "print(f\"Calculated number of steps: {num_steps}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "a6802973",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper function to get a batch of data\n",
                "@torch.no_grad()\n",
                "def get_batch():\n",
                "    assistant_end = tokenizer.encode_special(\"<|assistant_end|>\")\n",
                "    rank_indices = range(ddp_rank, len(train_task), ddp_world_size)\n",
                "    \n",
                "    iterator = itertools.cycle(rank_indices)\n",
                "    \n",
                "    while True:\n",
                "        example_idx = next(iterator)\n",
                "        \n",
                "        conversation = train_task[example_idx]\n",
                "        \n",
                "        tokens = tokenizer.render_for_completion(conversation)\n",
                "        prefix_length = len(tokens)\n",
                "        \n",
                "        model.eval()\n",
                "        generated_token_sequences = []\n",
                "        masks = []\n",
                "        \n",
                "        # batch gen so we dont oom\n",
                "        num_sampling_steps = max(1, num_samples // device_batch_size)\n",
                "        current_samples = 0\n",
                "        \n",
                "        for sampling_step in range(num_sampling_steps):\n",
                "            remaining = num_samples - current_samples\n",
                "            batch_size = min(device_batch_size, remaining)\n",
                "            if batch_size <= 0: break\n",
                "            \n",
                "            seed = (example_idx * 1000 + sampling_step) & 0x7FFFFFFF\n",
                "            \n",
                "            with autocast_ctx:\n",
                "                generated_token_sequences_batch, masks_batch = engine.generate_batch(\n",
                "                    tokens,\n",
                "                    num_samples=batch_size,\n",
                "                    max_tokens=max_new_tokens,\n",
                "                    temperature=temperature,\n",
                "                    top_k=top_k,\n",
                "                    seed=seed,\n",
                "                )\n",
                "            generated_token_sequences.extend(generated_token_sequences_batch)\n",
                "            masks.extend(masks_batch)\n",
                "            current_samples += batch_size\n",
                "            \n",
                "        rewards = []\n",
                "        for sample_tokens in generated_token_sequences:\n",
                "            generated_tokens = sample_tokens[prefix_length:]\n",
                "            generated_text = tokenizer.decode(generated_tokens)\n",
                "            # print(\"Gen text:\", generated_text)\n",
                "            reward = train_task.reward(conversation, generated_text)\n",
                "            rewards.append(reward)\n",
                "            \n",
                "        max_length = max(len(seq) for seq in generated_token_sequences)\n",
                "        padded_generated_token_sequences = [seq + [assistant_end] * (max_length - len(seq)) for seq in generated_token_sequences]\n",
                "        padded_masks = [mask + [0] * (max_length - len(mask)) for mask in masks]\n",
                "        \n",
                "        ids = torch.tensor(padded_generated_token_sequences, dtype=torch.long, device=device)\n",
                "        mask_ids = torch.tensor(padded_masks, dtype=torch.long, device=device)\n",
                "        \n",
                "        inputs = ids[:, :-1]\n",
                "        targets = ids[:, 1:].clone()\n",
                "        targets[mask_ids[:, 1:] == 0] = -1\n",
                "        \n",
                "        rewards = torch.tensor(rewards, dtype=torch.float, device=device)\n",
                "        \n",
                "        # GRPO Advantage Calculation: (r - mean(r)) / (std(r) + eps) or just (r - mean(r))\n",
                "        # The script uses just (r - mu)\n",
                "        mu = rewards.mean()\n",
                "        # std = rewards.std()\n",
                "        advantages = rewards - mu\n",
                "        # advantages = advantages / (std + 1e-8) # Optional: Normalize\n",
                "        \n",
                "        yield generated_token_sequences, inputs, targets, rewards, advantages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "id": "91f841ca",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Scaling the LR for the AdamW parameters ∝1/√(2048/768) = 0.612372\n"
                    ]
                }
            ],
            "source": [
                "# Setup Optimizer\n",
                "optimizers = model.setup_optimizers(\n",
                "    unembedding_lr=unembedding_lr,\n",
                "    embedding_lr=embedding_lr,\n",
                "    matrix_lr=matrix_lr,\n",
                "    weight_decay=weight_decay,\n",
                ")\n",
                "\n",
                "for opt in optimizers:\n",
                "    for group in opt.param_groups:\n",
                "        group[\"lr\"] = group[\"lr\"] * init_lr_frac\n",
                "        group[\"initial_lr\"] = group[\"lr\"]\n",
                "\n",
                "def get_lr_multiplier(it):\n",
                "    lrm = 1.0 - it / num_steps\n",
                "    return max(0.0, lrm)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "57e92e3c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting training...\n",
                        "--- Eval at step 0 ---\n",
                        "Step 0 | Mean Reward: 0.0625 | LR Multiplier: 1.0000\n",
                        "Step 1 | Mean Reward: 0.0000 | LR Multiplier: 0.9995\n",
                        "Step 2 | Mean Reward: 0.0000 | LR Multiplier: 0.9989\n",
                        "Step 3 | Mean Reward: 0.0625 | LR Multiplier: 0.9984\n",
                        "Step 4 | Mean Reward: 0.0000 | LR Multiplier: 0.9979\n",
                        "Step 5 | Mean Reward: 0.0000 | LR Multiplier: 0.9973\n",
                        "Step 6 | Mean Reward: 0.0625 | LR Multiplier: 0.9968\n",
                        "Step 7 | Mean Reward: 0.1250 | LR Multiplier: 0.9963\n",
                        "Step 8 | Mean Reward: 0.0000 | LR Multiplier: 0.9957\n",
                        "Step 9 | Mean Reward: 0.1250 | LR Multiplier: 0.9952\n",
                        "Step 10 | Mean Reward: 0.0000 | LR Multiplier: 0.9946\n",
                        "Step 11 | Mean Reward: 0.0000 | LR Multiplier: 0.9941\n",
                        "Step 12 | Mean Reward: 0.0625 | LR Multiplier: 0.9936\n",
                        "Step 13 | Mean Reward: 0.1250 | LR Multiplier: 0.9930\n",
                        "Step 14 | Mean Reward: 0.0000 | LR Multiplier: 0.9925\n",
                        "Step 15 | Mean Reward: 0.0000 | LR Multiplier: 0.9920\n",
                        "Step 16 | Mean Reward: 0.0625 | LR Multiplier: 0.9914\n",
                        "Step 17 | Mean Reward: 0.0000 | LR Multiplier: 0.9909\n",
                        "Step 18 | Mean Reward: 0.0000 | LR Multiplier: 0.9904\n",
                        "Step 19 | Mean Reward: 0.0000 | LR Multiplier: 0.9898\n",
                        "--- Eval at step 20 ---\n",
                        "Step 20 | Mean Reward: 0.0625 | LR Multiplier: 0.9893\n",
                        "Step 21 | Mean Reward: 0.0625 | LR Multiplier: 0.9888\n",
                        "Step 22 | Mean Reward: 0.0625 | LR Multiplier: 0.9882\n",
                        "Step 23 | Mean Reward: 0.0625 | LR Multiplier: 0.9877\n",
                        "Step 24 | Mean Reward: 0.0625 | LR Multiplier: 0.9872\n",
                        "Step 25 | Mean Reward: 0.0625 | LR Multiplier: 0.9866\n",
                        "Step 26 | Mean Reward: 0.0625 | LR Multiplier: 0.9861\n",
                        "Step 27 | Mean Reward: 0.1250 | LR Multiplier: 0.9855\n",
                        "Step 28 | Mean Reward: 0.0625 | LR Multiplier: 0.9850\n",
                        "Step 29 | Mean Reward: 0.0000 | LR Multiplier: 0.9845\n",
                        "Step 30 | Mean Reward: 0.0000 | LR Multiplier: 0.9839\n",
                        "Step 31 | Mean Reward: 0.0625 | LR Multiplier: 0.9834\n",
                        "Step 32 | Mean Reward: 0.0625 | LR Multiplier: 0.9829\n",
                        "Step 33 | Mean Reward: 0.0000 | LR Multiplier: 0.9823\n",
                        "Step 34 | Mean Reward: 0.0625 | LR Multiplier: 0.9818\n",
                        "Step 35 | Mean Reward: 0.1875 | LR Multiplier: 0.9813\n",
                        "Step 36 | Mean Reward: 0.0625 | LR Multiplier: 0.9807\n",
                        "Step 37 | Mean Reward: 0.1250 | LR Multiplier: 0.9802\n",
                        "Step 38 | Mean Reward: 0.0000 | LR Multiplier: 0.9797\n",
                        "Step 39 | Mean Reward: 0.1250 | LR Multiplier: 0.9791\n",
                        "--- Eval at step 40 ---\n",
                        "Step 40 | Mean Reward: 0.0625 | LR Multiplier: 0.9786\n",
                        "Step 41 | Mean Reward: 0.0625 | LR Multiplier: 0.9781\n",
                        "Step 42 | Mean Reward: 0.0000 | LR Multiplier: 0.9775\n",
                        "Step 43 | Mean Reward: 0.1875 | LR Multiplier: 0.9770\n",
                        "Step 44 | Mean Reward: 0.0000 | LR Multiplier: 0.9764\n",
                        "Step 45 | Mean Reward: 0.0000 | LR Multiplier: 0.9759\n",
                        "Step 46 | Mean Reward: 0.0000 | LR Multiplier: 0.9754\n",
                        "Step 47 | Mean Reward: 0.0000 | LR Multiplier: 0.9748\n",
                        "Step 48 | Mean Reward: 0.0625 | LR Multiplier: 0.9743\n",
                        "Step 49 | Mean Reward: 0.0000 | LR Multiplier: 0.9738\n",
                        "Step 50 | Mean Reward: 0.0625 | LR Multiplier: 0.9732\n",
                        "Step 51 | Mean Reward: 0.1875 | LR Multiplier: 0.9727\n",
                        "Step 52 | Mean Reward: 0.0000 | LR Multiplier: 0.9722\n",
                        "Step 53 | Mean Reward: 0.1250 | LR Multiplier: 0.9716\n",
                        "Step 54 | Mean Reward: 0.0000 | LR Multiplier: 0.9711\n",
                        "Step 55 | Mean Reward: 0.0000 | LR Multiplier: 0.9706\n",
                        "Step 56 | Mean Reward: 0.0000 | LR Multiplier: 0.9700\n",
                        "Step 57 | Mean Reward: 0.1250 | LR Multiplier: 0.9695\n",
                        "Step 58 | Mean Reward: 0.0625 | LR Multiplier: 0.9690\n",
                        "Step 59 | Mean Reward: 0.3750 | LR Multiplier: 0.9684\n",
                        "--- Eval at step 60 ---\n",
                        "Step 60 | Mean Reward: 0.0000 | LR Multiplier: 0.9679\n",
                        "Step 61 | Mean Reward: 0.0000 | LR Multiplier: 0.9673\n",
                        "Step 62 | Mean Reward: 0.1875 | LR Multiplier: 0.9668\n",
                        "Step 63 | Mean Reward: 0.0625 | LR Multiplier: 0.9663\n",
                        "Step 64 | Mean Reward: 0.1250 | LR Multiplier: 0.9657\n",
                        "Step 65 | Mean Reward: 0.1875 | LR Multiplier: 0.9652\n",
                        "Step 66 | Mean Reward: 0.0000 | LR Multiplier: 0.9647\n",
                        "Step 67 | Mean Reward: 0.0000 | LR Multiplier: 0.9641\n",
                        "Step 68 | Mean Reward: 0.0000 | LR Multiplier: 0.9636\n",
                        "Step 69 | Mean Reward: 0.1250 | LR Multiplier: 0.9631\n",
                        "Step 70 | Mean Reward: 0.0625 | LR Multiplier: 0.9625\n",
                        "Step 71 | Mean Reward: 0.0625 | LR Multiplier: 0.9620\n",
                        "Step 72 | Mean Reward: 0.0000 | LR Multiplier: 0.9615\n",
                        "Step 73 | Mean Reward: 0.1250 | LR Multiplier: 0.9609\n",
                        "Step 74 | Mean Reward: 0.0000 | LR Multiplier: 0.9604\n",
                        "Step 75 | Mean Reward: 0.0000 | LR Multiplier: 0.9599\n",
                        "Step 76 | Mean Reward: 0.0000 | LR Multiplier: 0.9593\n",
                        "Step 77 | Mean Reward: 0.3125 | LR Multiplier: 0.9588\n",
                        "Step 78 | Mean Reward: 0.0625 | LR Multiplier: 0.9582\n",
                        "Step 79 | Mean Reward: 0.0000 | LR Multiplier: 0.9577\n",
                        "--- Eval at step 80 ---\n",
                        "Step 80 | Mean Reward: 0.1875 | LR Multiplier: 0.9572\n",
                        "Step 81 | Mean Reward: 0.0625 | LR Multiplier: 0.9566\n",
                        "Step 82 | Mean Reward: 0.0000 | LR Multiplier: 0.9561\n",
                        "Step 83 | Mean Reward: 0.0000 | LR Multiplier: 0.9556\n",
                        "Step 84 | Mean Reward: 0.0625 | LR Multiplier: 0.9550\n",
                        "Step 85 | Mean Reward: 0.1250 | LR Multiplier: 0.9545\n",
                        "Step 86 | Mean Reward: 0.1250 | LR Multiplier: 0.9540\n",
                        "Step 87 | Mean Reward: 0.0000 | LR Multiplier: 0.9534\n",
                        "Step 88 | Mean Reward: 0.0625 | LR Multiplier: 0.9529\n",
                        "Step 89 | Mean Reward: 0.0625 | LR Multiplier: 0.9524\n",
                        "Step 90 | Mean Reward: 0.0625 | LR Multiplier: 0.9518\n",
                        "Step 91 | Mean Reward: 0.1250 | LR Multiplier: 0.9513\n",
                        "Step 92 | Mean Reward: 0.0000 | LR Multiplier: 0.9507\n",
                        "Step 93 | Mean Reward: 0.0625 | LR Multiplier: 0.9502\n",
                        "Step 94 | Mean Reward: 0.0625 | LR Multiplier: 0.9497\n",
                        "Step 95 | Mean Reward: 0.1250 | LR Multiplier: 0.9491\n",
                        "Step 96 | Mean Reward: 0.0625 | LR Multiplier: 0.9486\n",
                        "Step 97 | Mean Reward: 0.0625 | LR Multiplier: 0.9481\n",
                        "Step 98 | Mean Reward: 0.0625 | LR Multiplier: 0.9475\n",
                        "Step 99 | Mean Reward: 0.0000 | LR Multiplier: 0.9470\n",
                        "--- Eval at step 100 ---\n",
                        "Step 100 | Mean Reward: 0.1875 | LR Multiplier: 0.9465\n",
                        "Step 101 | Mean Reward: 0.0625 | LR Multiplier: 0.9459\n",
                        "Step 102 | Mean Reward: 0.0000 | LR Multiplier: 0.9454\n",
                        "Step 103 | Mean Reward: 0.1250 | LR Multiplier: 0.9449\n",
                        "Step 104 | Mean Reward: 0.0000 | LR Multiplier: 0.9443\n",
                        "Step 105 | Mean Reward: 0.1250 | LR Multiplier: 0.9438\n",
                        "Step 106 | Mean Reward: 0.0000 | LR Multiplier: 0.9433\n",
                        "Step 107 | Mean Reward: 0.0000 | LR Multiplier: 0.9427\n",
                        "Step 108 | Mean Reward: 0.0000 | LR Multiplier: 0.9422\n",
                        "Step 109 | Mean Reward: 0.1875 | LR Multiplier: 0.9416\n",
                        "Step 110 | Mean Reward: 0.0625 | LR Multiplier: 0.9411\n",
                        "Step 111 | Mean Reward: 0.1875 | LR Multiplier: 0.9406\n",
                        "Step 112 | Mean Reward: 0.0000 | LR Multiplier: 0.9400\n",
                        "Step 113 | Mean Reward: 0.1250 | LR Multiplier: 0.9395\n",
                        "Step 114 | Mean Reward: 0.0625 | LR Multiplier: 0.9390\n",
                        "Step 115 | Mean Reward: 0.0000 | LR Multiplier: 0.9384\n",
                        "Step 116 | Mean Reward: 0.0000 | LR Multiplier: 0.9379\n",
                        "Step 117 | Mean Reward: 0.1250 | LR Multiplier: 0.9374\n",
                        "Step 118 | Mean Reward: 0.0000 | LR Multiplier: 0.9368\n",
                        "Step 119 | Mean Reward: 0.0000 | LR Multiplier: 0.9363\n",
                        "--- Eval at step 120 ---\n",
                        "Step 120 | Mean Reward: 0.0625 | LR Multiplier: 0.9358\n",
                        "Step 121 | Mean Reward: 0.1250 | LR Multiplier: 0.9352\n",
                        "Step 122 | Mean Reward: 0.1250 | LR Multiplier: 0.9347\n",
                        "Step 123 | Mean Reward: 0.0000 | LR Multiplier: 0.9342\n",
                        "Step 124 | Mean Reward: 0.0000 | LR Multiplier: 0.9336\n",
                        "Step 125 | Mean Reward: 0.0625 | LR Multiplier: 0.9331\n",
                        "Step 126 | Mean Reward: 0.1875 | LR Multiplier: 0.9325\n",
                        "Step 127 | Mean Reward: 0.0625 | LR Multiplier: 0.9320\n",
                        "Step 128 | Mean Reward: 0.0000 | LR Multiplier: 0.9315\n",
                        "Step 129 | Mean Reward: 0.0625 | LR Multiplier: 0.9309\n",
                        "Step 130 | Mean Reward: 0.0000 | LR Multiplier: 0.9304\n",
                        "Step 131 | Mean Reward: 0.1250 | LR Multiplier: 0.9299\n",
                        "Step 132 | Mean Reward: 0.0000 | LR Multiplier: 0.9293\n",
                        "Step 133 | Mean Reward: 0.0625 | LR Multiplier: 0.9288\n",
                        "Step 134 | Mean Reward: 0.0000 | LR Multiplier: 0.9283\n",
                        "Step 135 | Mean Reward: 0.0000 | LR Multiplier: 0.9277\n",
                        "Step 136 | Mean Reward: 0.2500 | LR Multiplier: 0.9272\n"
                    ]
                }
            ],
            "source": [
                "print(\"Starting training...\")\n",
                "batch_iterator = get_batch() \n",
                "\n",
                "# num of steps to run\n",
                "steps_to_run = 250\n",
                "\n",
                "for step in range(steps_to_run):\n",
                "    if step % eval_every == 0:\n",
                "        print(f\"--- Eval at step {step} ---\")\n",
                "\n",
                "    rewards_list = []\n",
                "    \n",
                "    # calc gradients over the batch size 'examples_per_step'\n",
                "    \n",
                "    for example_step in range(examples_per_step):\n",
                "        sequences_all, inputs_all, targets_all, rewards_all, advantages_all = next(batch_iterator)\n",
                "        \n",
                "        model.train()\n",
                "        \n",
                "        # inputs_all is (num_samples, seq_len)\n",
                "        total_samples = inputs_all.size(0)\n",
                "        num_passes = (total_samples + device_batch_size - 1) // device_batch_size\n",
                "        \n",
                "        for pass_idx in range(num_passes):\n",
                "            b0 = pass_idx * device_batch_size\n",
                "            b1 = min((pass_idx + 1) * device_batch_size, total_samples)\n",
                "            \n",
                "            inputs = inputs_all[b0:b1]\n",
                "            targets = targets_all[b0:b1]\n",
                "            advantages = advantages_all[b0:b1]\n",
                "            \n",
                "            with autocast_ctx:\n",
                "                # loss_reduction='none' gives (B, T)\n",
                "                logp = -model(inputs, targets, loss_reduction='none').view_as(inputs)\n",
                "            \n",
                "            #policy gradient: - sum(log_prob * advantage)\n",
                "            # mask out invalid tokens (targets == -1)\n",
                "            pg_obj = (logp * advantages.unsqueeze(-1)).sum()\n",
                "            \n",
                "            num_valid = (targets >= 0).sum().clamp(min=1)\n",
                "            # avg over the total number of examples we are processing in this step\n",
                "            pg_obj = pg_obj / (num_valid * num_passes * examples_per_step) # normalize objective\n",
                "            \n",
                "            loss = -pg_obj\n",
                "            loss.backward()\n",
                "            \n",
                "        rewards_list.append(rewards_all.mean().item())\n",
                "        \n",
                "    # model weight updates\n",
                "    lrm = get_lr_multiplier(step)\n",
                "    for opt in optimizers:\n",
                "        for group in opt.param_groups:\n",
                "            group[\"lr\"] = group[\"initial_lr\"] * lrm\n",
                "            \n",
                "    for opt in optimizers:\n",
                "        opt.step()\n",
                "    model.zero_grad(set_to_none=True)\n",
                "    \n",
                "    mean_reward = sum(rewards_list) / len(rewards_list)\n",
                "    print(f\"Step {step} | Mean Reward: {mean_reward:.4f} | LR Multiplier: {lrm:.4f}\")\n",
                "    \n",
                "    if use_wandb:\n",
                "        wandb.log({\"step\": step, \"reward\": mean_reward, \"lrm\": lrm})\n",
                "\n",
                "    # 5. Save Model\n",
                "    if (step > 0 and step % save_every == 0) or step == steps_to_run - 1:\n",
                "        base_dir = get_base_dir()\n",
                "        depth = model.config.n_layer\n",
                "        model_tag = f\"d{depth}\" # base the model tag on the depth of the base model\n",
                "        checkpoint_dir = os.path.join(base_dir, \"chatrl_checkpoints\", model_tag)\n",
                "        model_config_kwargs = model.config.__dict__\n",
                "        save_checkpoint(\n",
                "            checkpoint_dir,\n",
                "            step,\n",
                "            model.state_dict(),\n",
                "            None, # note: we don't bother to save the optimizer state\n",
                "            {\n",
                "                \"model_config\": model_config_kwargs,\n",
                "            }\n",
                "        )\n",
                "        print(f\"✅ Saved model checkpoint to {checkpoint_dir}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7d89ffd1",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nanochat-rl",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
